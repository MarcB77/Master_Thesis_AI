{"cells":[{"cell_type":"markdown","source":["# Installs op cluster:\n\n!pip install torch\n\nclick==8.0.4\nkeras\ntensorflow\nspacy \nscispacy\nnltk \ntorch"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5411a91f-4189-4a91-8e7b-606129f7c1df"}}},{"cell_type":"markdown","source":["# Libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be23ad15-e50f-44ac-ab27-8b37d541575c"}}},{"cell_type":"code","source":["import pandas as pd\nimport os\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.functions import col\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport torch\nimport numpy as np\nfrom collections import Counter\nfrom tqdm.keras import TqdmCallback\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report, accuracy_score\n\n%matplotlib inline\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4284678d-77e0-471b-8348-15611fe857f8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Tables from Database"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3edf2c2-3fbd-49ef-9f0d-994384293d55"}}},{"cell_type":"code","source":["#basis + nutrienten + food\nbasis = spark.read.format('delta').load('/mnt/Prd_adls/Conformed/TIMS/TradeItem/ahTradeItem/Data/').filter('__DeletedFlag == 0')\nbasis = basis.select(\"gtin\", \"gln\", 'glnAH', 'sgIngredientsRequiredIndicator', 'ndSingleComposed')\n\nfood = spark.sql(\"select gtin, gln, ndFoodNonFood from ndattribuutoutput.ndfoodnonfood\")\n\nvezelwijzer = spark.sql(\"select gtin, gln, Preferred_table, FIBER, Fiber_UOM, NBQ_UOM, SUGAR, SUGAR_UOM, FAT, FASAT, CHOAVL, PROTEINE, SALT, ENER_KJ, ENER_Kcal, Volgorde_Nutrienten from standaardtabellen. Nutrienten\") # NBQ_value\nvezelwijzer = vezelwijzer.filter(((vezelwijzer.Volgorde_Nutrienten == '0') | \n                                  (vezelwijzer.Volgorde_Nutrienten == '1') | \n                                  (vezelwijzer.Volgorde_Nutrienten == '2') | \n                                  (vezelwijzer.Volgorde_Nutrienten == '3')) & \n                                 (vezelwijzer.Preferred_table == \"Ja\"))\nvezelwijzer = vezelwijzer.drop('Volgorde_Nutrienten', 'Preferred_table')\nvezelwijzer = vezelwijzer.drop_duplicates()\n\ndf = basis.join(food, ['gtin', 'gln'], how = \"left\")\ndf = df.join(vezelwijzer, ['gtin', 'gln'], how = \"left\")\n\n#df nlp spark\nDF_NLP_spark = spark.sql(\"select gtin, gln, lemmatized_DUTCH_EN as Lemmatized, padded_seq__EN, padded_seq_ from default._paddedseq_ingredient\")\n\nDF_NLP_spark = DF_NLP_spark.join(df, ['gtin', 'gln'], how = \"left\")\nDF_NLP_spark = DF_NLP_spark.filter((DF_NLP_spark.glnAH == True) & (DF_NLP_spark.ndFoodNonFood == 'Food'))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e44288b-c060-44a9-8663-d8ea2a984c00"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Koppel ndAttributen\ndef koppel_ndAttributen():\n    df_controle = spark.sql(\"select * from default._ndoutputattributen_25_3_2022\") # Verander deze met Vernieuwde lijst?\n    nd_controle = df_controle.join(NASA, [\"gtin\"])\n    nd_controle = nd_controle.filter(nd_controle.glnAH == True)\n    print(\"Amount of rows: \",(nd_controle.count(),\"Amount of columns: \",len(nd_controle.columns))) # Shape of dataframe\n    return nd_controle\n\ndef koppel_preprocessed_text(df_geformateerd):\n    df_geformateerd = spark.createDataFrame(df_geformateerd)\n    DF_merged_ = (DF_NLP_spark.join(df_geformateerd, [\"gtin\", \"gln\",\"glnAH\"], how=\"left\"))  # verander hier table naar bovengenoemde attribuut, zie notebook --> Functions\n\n    DF_merged_ = DF_merged_.drop_duplicates()\n    spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")\n    DF_merged = DF_merged_.toPandas()\n    print(\"Shape van dataframe: \",(DF_merged[\"gln\"].count(),len(DF_merged.columns)))\n    return DF_merged\n    \ndef fix_empty_fields(DF_merged, ATTRIBUUT):\n    # Fix empty fields van class_num\n    print(\"null entries = \", DF_merged[\"class_num_\"+str(ATTRIBUUT)].isna().sum())\n    # class_empty = DF_merged[DF_merged[str(ATTRIBUUT)].str.match('EMPTY', na=False)]\n    # class_empty = class_empty[\"class_num_\"+str(ATTRIBUUT)].tolist()\n\n    # DF_merged.loc[DF_merged[\"class_num_\"+str(ATTRIBUUT)].isnull(),[\"class_num_\"+str(ATTRIBUUT)]] = DF_merged.loc[DF_merged[\"class_num_\"+str(ATTRIBUUT)].isnull(),\"class_num_\"+str(ATTRIBUUT)].apply(lambda x: list(class_empty[0]))\n    DF_merged = DF_merged[DF_merged[\"class_num_\"+str(ATTRIBUUT)].notna()]\n    #print(DF_merged[\"class_num_\"+str(ATTRIBUUT)].isna().sum())\n    return DF_merged\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff155539-0067-41f5-a5de-c5b3b160d16d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c9a1647-9504-4565-b85d-99ea733b9a2c"}}},{"cell_type":"markdown","source":["## Correlatie matrix, Pass Numerical Features"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7acc6053-cf94-4211-b52f-b55f26bccc57"}}},{"cell_type":"code","source":["def Correlation_matrix(DF_merged, ATTRIBUUT, show_nan_plot=True):\n    # numerieke features met empty entries geven error op de correlatie tabel\n    plt.rcParams.update({'font.size': 11})\n    NUMERIEKE_nutrienten = [\"FIBER\",\"SUGAR\",\"FAT\",\"FASAT\",\"CHOAVL\",\"PROTEINE\",\"SALT\",\"ENER_KJ\",\"ENER_Kcal\"]\n    DF_merged[\"CLASS_\"+str(ATTRIBUUT)] = DF_merged[\"class_num_\"+str(ATTRIBUUT)].apply(np.argmax)\n    \n    nan_list_numeriek = []\n    nan_list_labels = []\n    for nutrient in NUMERIEKE_nutrienten:\n        DF_merged[nutrient] = DF_merged[nutrient].astype(float)\n        #print(nutrient, DF_merged[nutrient].isna().sum())\n        nan_list_numeriek.append(DF_merged[nutrient].isna().sum())\n        nan_list_labels.append(nutrient)\n        #DF_merged[nutrient] = DF_merged[nutrient].fillna(0)\n        DF_merged[nutrient+\"_was_missing\"] = np.where(DF_merged[nutrient].isnull(), 1, 0)\n        if nutrient == \"SUGAR\":\n            DF_merged[\"SUGAR_IS_ZERO\"] = np.where(DF_merged[nutrient]== 0.0, 1, 0)\n        # REPLACE the null now by a mean\n        DF_merged[nutrient] = DF_merged[nutrient].fillna(DF_merged[nutrient].mean()) # mean or median\n#     if show_nan_plot == True: \n#         xs = np.arange(len(nan_list_labels)) \n#         width = 1\n#         plt.bar(xs, nan_list_numeriek, width, align='center')\n#         plt.xticks(xs, nan_list_labels, rotation=65, ha=\"right\") \n#         #plt.yticks(nan_list_numeriek)\n#         #plt.legend()\n#         plt.show()\n    if show_nan_plot == True: \n        fig, ax = plt.subplots(figsize =(16, 9))\n        bars = ax.barh(nan_list_labels, nan_list_numeriek)        \n        ax.set_title('Null values in nutritional attributes.\\nBased on Food products.',\n                     loc ='center', )\n        ax.bar_label(bars, label_type='center', fontsize=20)\n        print(bars, type(bars))\n        # Show Plot\n        plt.show()\n    # string features omzetten naar numerieke classes\n    from sklearn import preprocessing\n    le = preprocessing.LabelEncoder()\n    le.fit(DF_merged['Fiber_UOM'])\n    DF_merged['Fiber_UOM_class']=le.transform(DF_merged['Fiber_UOM'])\n    le.fit(DF_merged['NBQ_UOM'])\n    DF_merged['NBQ_UOM_class']=le.transform(DF_merged['NBQ_UOM'])\n    le.fit(DF_merged['SUGAR_UOM'])\n    DF_merged['SUGAR_UOM_class']=le.transform(DF_merged['SUGAR_UOM'])\n\n    #display(DF_merged.head())\n    df_correlatie = pd.DataFrame(DF_merged,columns=[\"FIBER\",\"SUGAR\",\"FAT\",\"FASAT\",\"CHOAVL\",\"PROTEINE\",\"SALT\",                                            \"ENER_KJ\",\"Fiber_UOM_class\",\"SUGAR_UOM_class\",\"NBQ_UOM_class\",'CLASS_'+str(ATTRIBUUT)])\n    f = plt.figure(figsize=(15,11))\n    corrMatrix = df_correlatie.corr()\n    sns.heatmap(corrMatrix, annot=True, fmt='.2f', cmap='Blues',annot_kws={\"fontsize\":16})\n    plt.show()\n    return DF_merged"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10250145-61f4-47f6-aa1b-a90ac1c83556"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Prepare Train, Test en validatie data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"722a3bd1-435c-439f-98cc-99184ccc21f8"}}},{"cell_type":"code","source":["# Formateer Features\n\ndef Numerical_features(ATTRIBUUT, DF_merged):\n    #Numerical_features = DF_merged[['Fiber_UOM_class','NBQ_UOM_class','SUGAR_UOM_class']]\n    Numerical_features = DF_merged[['Fiber_UOM_class','NBQ_UOM_class','SUGAR_UOM_class']]\n    # Welke numerical features wil je bij het attribuut?\n    if ATTRIBUUT == \"ndFiberIndex\":\n        Numerical_features = DF_merged[['FIBER','FIBER_was_missing','Fiber_UOM_class','PROTEINE']] # ndFiberIndex\n    if ATTRIBUUT == \"ndAddedSugar\":\n        Numerical_features = DF_merged[['SUGAR_was_missing','SUGAR_UOM_class','SUGAR']]\n    if ATTRIBUUT == \"ndFreeOfAlcohol\":\n        Numerical_features = DF_merged[['Fiber_UOM_class','NBQ_UOM_class','SUGAR_UOM_class']]\n    if ATTRIBUUT == \"ndTypeOfGrain\":\n        Numerical_features = ['FIBER']\n    if ATTRIBUUT == \"ndAnimalSpecies\":\n        Numerical_features = ['FIBER']\n    return Numerical_features\n\ndef Formateer_Features(DF_merged, ATTRIBUUT):\n    X_train_text_DF = DF_merged['padded_seq__EN'].tolist()\n    X_train_text = torch.FloatTensor(X_train_text_DF)\n    X_train_text = X_train_text.numpy()\n    vocab_size = np.amax(X_train_text)+1\n\n    # TARGET\n    ytrain = DF_merged['class_num_'+ATTRIBUUT].tolist()\n    ytrain_tensor = torch.FloatTensor(ytrain)\n    ytrain = ytrain_tensor.numpy()\n    print(\"Vocabular size: \", vocab_size)\n    return vocab_size, X_train_text, ytrain"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ddb0a48-d827-4ffd-b44b-d149c22d4a59"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def split_train_test(xtrain, ytrain):\n    from sklearn.model_selection import train_test_split\n    Xtrain, Xval, ytrain, yval = train_test_split(xtrain, ytrain, test_size=0.3, random_state = 42)\n    Xval, Xtest, yval, ytest = train_test_split(Xval, yval, test_size=0.5, random_state = 42)\n    return Xtrain, ytrain, Xval, yval, Xtest, ytest\n\ndef unzip_sets(Xtrain, Xval, Xtest):\n    Xtrain_num = Xtrain[:,0]\n    Xtrain_text = Xtrain[:,1]\n    Xval_num = Xval[:,0]\n    Xval_text = Xval[:,1]\n    Xtest_num = Xtest[:,0]\n    Xtest_text = Xtest[:,1]\n    return Xtrain_num, Xtrain_text, Xval_num, Xval_text, Xtest_num, Xtest_text\n\ndef to_tensor(Xtrain_num, Xtrain_text, Xval_num, Xval_text, Xtest_num, Xtest_text):\n    Xtrain_num = Xtrain_num.tolist()\n    Xtrain_num = torch.FloatTensor(Xtrain_num)\n    Xtrain_num = Xtrain_num.numpy()\n\n    Xval_num = Xval_num.tolist()\n    Xval_num = torch.FloatTensor(Xval_num)\n    Xval_num = Xval_num.numpy()\n\n    Xtest_num = Xtest_num.tolist()\n    Xtest_num = torch.FloatTensor(Xtest_num)\n    Xtest_num = Xtest_num.numpy()\n\n    Xtrain_text = Xtrain_text.tolist()\n    Xtrain_text = torch.FloatTensor(Xtrain_text)\n    Xtrain_text = Xtrain_text.numpy()\n\n    Xval_text = Xval_text.tolist()\n    Xval_text = torch.FloatTensor(Xval_text)\n    Xval_text = Xval_text.numpy()\n\n    Xtest_text = Xtest_text.tolist()\n    Xtest_text = torch.FloatTensor(Xtest_text)\n    Xtest_text = Xtest_text.numpy()\n    return  Xtrain_num, Xtrain_text, Xval_num, Xval_text, Xtest_num, Xtest_text\n  \ndef multiple_classes_PREP(numerical_ftrs, X_train, X_test, X_val, y_train, y_test, y_val, target):\n    X_train_num = X_train[numerical_ftrs].to_numpy()\n    X_test_num = X_test[numerical_ftrs].to_numpy()\n    X_val_num = X_val[numerical_ftrs].to_numpy()\n\n    X_train_text = X_train['padded_seq__EN'].tolist()\n    X_train_text = torch.FloatTensor(X_train_text)\n    X_train_text = X_train_text.numpy()\n    \n    X_test_text = X_test['padded_seq__EN'].tolist()\n    X_test_text = torch.FloatTensor(X_test_text)\n    X_test_text = X_test_text.numpy()\n    \n    X_val_text = X_val['padded_seq__EN'].tolist()\n    X_val_text = torch.FloatTensor(X_val_text)\n    X_val_text = X_val_text.numpy()\n    \n    # TARGET\n    ytrain = y_train[target].tolist()\n    ytrain_tensor = torch.FloatTensor(ytrain)\n    ytrain = ytrain_tensor.numpy()\n\n    ytest = y_test[target].tolist()\n    ytest_tensor = torch.FloatTensor(ytest)\n    ytest = ytest_tensor.numpy()\n\n    yval = y_val[target].tolist()\n    yval_tensor = torch.FloatTensor(yval)\n    yval = yval_tensor.numpy()\n\n    return X_train_num, X_test_num, X_val_num, X_train_text, X_test_text, X_val_text, ytrain, ytest, yval\n  \ndef single_prep(numerieke_features, DF_merged, ATTRIBUUT):\n    Numerical_features = DF_merged[numerieke_features]\n\n    X_train_num = Numerical_features.to_numpy()\n\n    vocab_size, X_train_text, ytrain = Formateer_Features(DF_merged, ATTRIBUUT)\n    print(\"Numerical Features: \", Numerical_features.columns)\n    print(\"Shape Numerical Features: \", X_train_num.shape)\n    print(\"Shape Text Features: \", X_train_text.shape)\n    print(\"Shape Targets: \", ytrain.shape)\n\n    X_train = np.array(list(zip(X_train_num, X_train_text)))\n    Xtrain, ytrain, Xval, yval, Xtest, ytest = split_train_test(X_train, ytrain)\n    print(Xtrain.shape, Xtest.shape, Xval.shape)\n    print(ytrain.shape, ytest.shape, yval.shape)\n    \n    Xtrain_num, Xtrain_text, Xval_num, Xval_text, Xtest_num, Xtest_text = unzip_sets(Xtrain, Xval, Xtest)\n    Xtrain_num, Xtrain_text, Xval_num, Xval_text, Xtest_num, Xtest_text = to_tensor(Xtrain_num, Xtrain_text, Xval_num, \n                                                                                Xval_text, Xtest_num, Xtest_text)# To Tensor format\n    return ytrain, yval, ytest, Xtrain_num, Xtrain_text, Xval_num, Xval_text, Xtest_num, Xtest_text\n  \ndef single_prep_controle(numerieke_features, DF_merged, ATTRIBUUT):\n    Numerical_features = DF_merged[numerieke_features]\n\n    X_train_num = Numerical_features.to_numpy()\n\n    vocab_size, X_train_text, ytrain = Formateer_Features(DF_merged, ATTRIBUUT)\n    print(\"Numerical Features: \", Numerical_features.columns)\n    print(\"Shape Numerical Features: \", X_train_num.shape)\n    print(\"Shape Text Features: \", X_train_text.shape)\n    print(\"Shape Targets: \", ytrain.shape)\n\n    return X_train_num, X_train_text, ytrain, vocab_size"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a161c1e-3f65-44b6-bc83-27c24fbafa97"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Model metrics, statistics and plots"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4fd304ee-4556-4e15-825d-58a6fc8c8ca2"}}},{"cell_type":"code","source":["def predictions(model):\n    yhat =  model.predict(Xtest, verbose=1)\n    yhat2 = np.round(yhat, 0)\n    model.evaluate(Xtest, ytest)\n\n    list_ytest = []\n    list_yhat = []\n    for idx in range(len(yhat2)):\n        list_ytest.append(np.argmax(ytest[idx]))\n        list_yhat.append(np.argmax(yhat2[idx]))\n    return list_ytest, list_yhat, yhat2\n\ndef accuracy_report(y_test, y_pred):\n    print(\"Confusion Matrix:\\n**************************\\n \",\n         confusion_matrix(y_test, y_pred),\"\\n**************************\\n\")\n    print(\"Accuracy: \",\n         accuracy_score(y_test, y_pred)*100)\n    print(\"Classification report: \",\n         classification_report(y_test, y_pred))\n    \ndef plot_confusion_matrix(DF_merged, cf_matrix, ATTRIBUUT):\n    unique_classes = DF_merged[\"class_num_\"+str(ATTRIBUUT)].tolist()\n    unique_classes = [item for item in unique_classes]\n    unique_classes = unique_classes[0]\n    print(\"unique amount of classes: \",len(unique_classes), unique_classes)\n    group_names = []\n    for i in range(len(unique_classes)):\n        for class_ in unique_classes: \n            group_names.append(class_)\n\n    group_counts = [\"{0:0.0f}\".format(value) for value in\n                    cf_matrix.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in\n                         cf_matrix.flatten()/np.sum(cf_matrix)]\n\n    labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n              zip(group_counts,group_percentages)]\n    \n    class_names= DF_merged[\"class_names_\"+str(ATTRIBUUT)].tolist()\n    \n    \n    labels = np.asarray(labels).reshape(len(unique_classes), len(unique_classes))\n    s = sns.heatmap(cf_matrix, xticklabels=class_names[0], yticklabels=class_names[0], annot=labels, fmt='', cmap='Blues')\n    s.set(xlabel='Predicted', ylabel='True label')\n    \n    s.set_title(\"Attribute:\\n \"+str(ATTRIBUUT)+\"\\n\"+str(' - '.join(class_names[0])))\n    plt.show()\n    \ndef plot_training(acc, val_acc, NAME='Accuracy'):\n    EPOCH = len(acc)\n    for param in ['figure.facecolor', 'axes.facecolor', 'savefig.facecolor']:\n        plt.rcParams[param] = '1.0'#'#212946'  # bluish dark grey\n    for param in ['text.color', 'axes.labelcolor', 'xtick.color', 'ytick.color']:\n        plt.rcParams[param] = '0.3'  # very light grey\n    epochs_range = np.arange(1,EPOCH+1)\n    plt.plot(epochs_range, acc, 'g', label='Train '+str(NAME))\n    plt.plot(epochs_range, val_acc, 'b', label='Val '+str(NAME))\n    plt.title('Training and Validation \\n '+str(NAME)+' of '+str(ATTRIBUUT))\n    plt.xlabel('Epochs')\n    plt.ylabel(str(NAME))\n    plt.legend()\n    plt.show()\n    \ndef predictions_2inputs(model, Xtest_num,Xtest_text, ytest):\n    yhat =  model.predict([Xtest_num, Xtest_text], verbose=1)\n    yhat2 = np.round(yhat, 0)\n    model.evaluate([Xtest_num, Xtest_text], ytest)\n\n    list_ytest = []\n    list_yhat = []\n    for idx in range(len(yhat2)):\n        list_ytest.append(np.argmax(ytest[idx]))\n        list_yhat.append(np.argmax(yhat2[idx]))\n    return list_ytest, list_yhat, yhat2\n\ndef plot_multiple_training(acc, val_acc, class_names, ATTRIBUUT):\n    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(25,14))\n    EPOCH = len(acc[0])\n    for param in ['figure.facecolor', 'axes.facecolor', 'savefig.facecolor']:\n        plt.rcParams[param] = '1.0'\n    for param in ['text.color', 'axes.labelcolor', 'xtick.color', 'ytick.color']:\n        plt.rcParams[param] = '0.3'  \n    epochs_range = np.arange(1,EPOCH+1)\n    for index, class_acc in enumerate(acc):\n        ax1.plot(epochs_range, class_acc, label=str(class_names[index]))\n    for index, class_val_acc in enumerate(val_acc):\n        ax2.plot(epochs_range, class_val_acc, label=str(class_names[index]))\n    ax1.set_title('Training \\n Accuracy of '+str(ATTRIBUUT))\n    ax2.set_title('Validation \\n Accuracy of '+str(ATTRIBUUT))\n    ax1.set_xlabel('Epochs')\n    ax1.set_ylabel('Accuracy')\n    ax2.set_xlabel('Epochs')\n    ax2.set_ylabel('Accuracy')\n    ax1.legend()\n    ax2.legend()\n    plt.show()\n    \ndef plot_single_label_training(model, model_history, Xtest_num, Xtest_text, ytest, ATTRIBUUT, DF_merged):\n    print(model_history.history.keys())\n \n    list_ytest, list_yhat, yhat2 = predictions_2inputs(model, Xtest_num, Xtest_text, ytest)\n\n    plot_confusion_matrix(DF_merged, confusion_matrix(list_ytest, list_yhat), ATTRIBUUT)\n    accuracy_report(list_ytest, list_yhat)\n    plot_training(model_history.history['accuracy'], model_history.history['val_accuracy'],NAME='Accuracy')\n    plot_training(model_history.history['loss'], model_history.history['val_loss'],NAME='Loss')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf0f7284-82b1-4b35-8c90-74702b0c5e62"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## vocabulary"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d021163-8d64-468c-99da-411a9fd9379e"}}},{"cell_type":"code","source":["def get_vocabulary_size(DF_ATTRIBUUT):\n    vocab = DF_ATTRIBUUT['padded_seq__EN'].tolist()\n    vocab = torch.FloatTensor(vocab)\n    vocab = vocab.numpy()\n    vocab_size = np.amax(vocab)+1\n    print(\"Vocab size: \", vocab_size)\n    return vocab_size"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df6deb09-bf22-4dd8-bc8e-0ce4b2e33128"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Neural network"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b328e67-8901-4c26-84f5-e7ba4fdd4f03"}}},{"cell_type":"code","source":["import keras\nfrom keras.models import Model\nfrom keras.layers import Dense, Dropout, Flatten, Embedding, GlobalAveragePooling1D, Input, concatenate, LSTM\nfrom tensorflow.keras.optimizers import Adam, SGD\n\ndef LOAD_MODEL(Xtrain_num, Xtrain_text, vocab_size, ytrain):\n    input_NUM = Input(shape=Xtrain_num.shape[1]) # Numerieke features\n    input_TEXT = Input(shape=Xtrain_text.shape[1]) # Text features\n\n    emb = Embedding(int(vocab_size), output_dim=20,input_length=Xtrain_text.shape[1])(input_TEXT)\n    #lstm = LSTM(8, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(emb)\n    fltn = Flatten()(emb)\n    x = concatenate([fltn, input_NUM])\n    \n    x = Dense(30, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(20, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(ytrain.shape[1], activation='softmax')(x)\n\n    model = Model(inputs=[input_NUM , input_TEXT], outputs=[x])\n    model.summary()\n\n\n    optimizer = SGD(learning_rate=0.01)\n    #model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n    return model\n  \ndef LOAD_MODEL_2(Xtrain_num, Xtrain_text, vocab_size, ytrain):\n    input_NUM = Input(shape=Xtrain_num.shape[1]) # Numerieke features\n    input_TEXT = Input(shape=Xtrain_text.shape[1]) # Text features\n\n    emb = Embedding(int(vocab_size), output_dim=4,input_length=Xtrain_text.shape[1])(input_TEXT)\n    #lstm = LSTM(8, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(emb)\n    emb = Flatten()(emb)\n    input_NUM = Dropout(0.3)(input_NUM)\n    emb = Dropout(0.3)(emb)\n    x = concatenate([emb, input_NUM])\n    x = Dense(30, activation='relu')(x)\n    x = Dense(50, activation='relu')(x)\n    x = Dense(20, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(ytrain.shape[1], activation='softmax')(x)\n\n    model = Model(inputs=[input_NUM , input_TEXT], outputs=[x])\n    model.summary()\n\n\n    optimizer = SGD(learning_rate=0.008)\n    #model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n    return model\n\ndef LOAD_MODEL_3(Xtrain_num, Xtrain_text, vocab_size, ytrain):\n    input_NUM = Input(shape=Xtrain_num.shape[1]) # Numerieke features\n    input_TEXT = Input(shape=Xtrain_text.shape[1]) # Text features\n\n    emb = Embedding(int(vocab_size), output_dim=4,input_length=Xtrain_text.shape[1])(input_TEXT)\n    #lstm = LSTM(8, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(emb)\n    emb = Flatten()(emb)\n    input_NUM = Dropout(0.3)(input_NUM)\n    emb = Dropout(0.3)(emb)\n    x = concatenate([emb, input_NUM])\n    x = Dense(100, activation='relu')(x)\n    x = Dense(50, activation='relu')(x)\n    x = Dense(100, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(ytrain.shape[1], activation='softmax')(x)\n\n    model = Model(inputs=[input_NUM , input_TEXT], outputs=[x])\n    model.summary()\n\n\n    optimizer = SGD(learning_rate=0.008)\n    #model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n    return model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b800dba-34ce-4a63-a928-74aaa07fd036"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Train model multi-label"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df7a5768-2e4b-474c-ac82-c95791a1b95f"}}},{"cell_type":"code","source":["def total_split(DF_merged, numerieke_features, ATTRIBUUT):\n    from sklearn.model_selection import train_test_split\n    X = DF_merged[['gtin','gln','Lemmatized',ATTRIBUUT,'padded_seq__EN','class_names_'+ATTRIBUUT]]\n    X2 = DF_merged[numerieke_features]\n    X = X.merge(X2, left_index=True, right_index=True, how='inner')\n    targets = DF_merged['class_names_'+ATTRIBUUT].tolist()\n    targets = targets[0]\n    y = DF_merged[targets]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                        test_size=0.2, random_state=1)\n\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n                                                      test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n\n    print(X_train.shape, X_test.shape, X_val.shape)\n    print(y_train.shape, y_test.shape, y_val.shape)\n    return  X_train, X_val, X_test, y_train, y_val, y_test, targets"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84e051c3-8dd4-4a14-80e9-2378982de767"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def TRAINING_multi_label(X_train, X_val, X_test, y_train, y_val, y_test, numerical_ftrs, vocab_size, ATTRIBUUT, SAVE_MODEL):\n    total_pred = []\n    total_test = []\n    train_accuracies = []\n    val_accuracies = []\n    class_names = []\n    print(\"**************** MULTI LABEL MODEL ***************\\n\")\n    print(\"Save all models = \",SAVE_MODEL,\"\\nNumerieke features = \", numerical_ftrs, \"\\nAttribuut = \", ATTRIBUUT )\n    \n    for (target, data) in tqdm(y_train.iteritems()):\n        print('\\n',target)\n        Xtrain_num, Xtest_num, Xval_num, Xtrain_text, Xtest_text, Xval_text, ytrain, ytest, yval = multiple_classes_PREP(numerical_ftrs, X_train, X_test, X_val, y_train, y_test, y_val, target)\n\n        print(Xtrain_num.shape, Xtest_num.shape, Xval_num.shape, Xtrain_text.shape, Xtest_text.shape, Xval_text.shape)\n        print(ytrain.shape, ytest.shape, yval.shape)\n        model = LOAD_MODEL_3(Xtrain_num, Xtrain_text, vocab_size, ytrain)\n\n        model_history = model.fit([Xtrain_num,Xtrain_text], ytrain, epochs=30, verbose=2, validation_data=([Xval_num,Xval_text], yval), shuffle=True)\n        list_ytest, list_yhat, yhat2 = predictions_2inputs(model, Xtest_num, Xtest_text, ytest)\n        accuracy_report(list_ytest, list_yhat) \n\n        #controle_tabel['TRUE '+str(target)] = list_ytest\n        #controle_tabel['PRED '+str(target)] = list_yhat\n        #total_test.append(list_ytest)\n        #total_pred.append(list_yhat)\n\n        train_accuracies.append(model_history.history['accuracy'])\n        val_accuracies.append(model_history.history['val_accuracy'])\n        class_names.append(target)\n        if SAVE_MODEL == True:\n            model.save(\"/FileStore/nd_MODELS_publicatie/\"+str(ATTRIBUUT)+\"/model_\"+str(target)+\".h5\")\n            dbutils.fs.cp(\"file:/FileStore/nd_MODELS_publicatie/\"+str(ATTRIBUUT)+\"/model_\"+str(target)+\".h5\", \"dbfs:/FileStore/nd_MODELS_publicatie/\"+str(ATTRIBUUT)+\"/model_\"+str(target)+\".h5\") \n    return train_accuracies, val_accuracies, class_names, model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c3b5904-b3cf-4511-affe-5a3b11ae946d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def TRAINING_single_label( Xtrain_num, Xtrain_text, Xval_num, Xval_text, Xtest_num, Xtest_text, vocab_size, EPOCHS, SAVE_MODEL):\n    print(\"**************** SINGLE LABEL MODEL ***************\\n\")\n    model = LOAD_MODEL_2(Xtrain_num, Xtrain_text, vocab_size, ytrain)\n    model_history = model.fit([Xtrain_num,Xtrain_text], ytrain, epochs=EPOCHS, verbose=2, validation_data=([Xval_num,Xval_text], yval), shuffle=True, batch_size=32) # Start training the model --> Singel label\n    \n    if SAVE_MODEL == True:\n        model.save(\"/FileStore/nd_MODELS_publicatie/model_\"+str(ATTRIBUUT)+\".h5\")\n        dbutils.fs.cp(\"file:/FileStore/nd_MODELS_publicatie/model_\"+str(ATTRIBUUT)+\".h5\", \"dbfs:/FileStore/nd_MODELS_publicatie/model_\"+str(ATTRIBUUT)+\".h5\") \n        display(dbutils.fs.ls(\"/FileStore/nd_MODELS_publicatie/model_\"+str(ATTRIBUUT)+\".h5\"))\n    return model, model_history"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee99436c-036f-4199-b403-8685c20eb19f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Functions_Rebuild","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":260378592162304}},"nbformat":4,"nbformat_minor":0}
